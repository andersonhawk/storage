### Chapter 0: Introduction ###
	ZFS is not just a filesystem, it is a combination filesystem and volume manager.
	ZFS being aware of exactly where data is going on the disk, can arrange files
	and stripes optimally, from top to bottom. ZFS can use secondary fast storage
	as special-purpose caches, further enhancing performance.


### Chapter 1: Introducing ZFS ###
	The ZFS stack include three layers: filesystem, storage pools, and virtual
	devices. ZFS tightly integrates the filesystem and the lower storage layers.
	This means it can dynamically divide storage space between the various
	filesystems as needed.

	Storage Pools (zpool)
	ZFS uses storage pools rather than disks. A storage pool is an abstraction
	atop the underlying storage providers, letting you seperate the physical
	medium and the user-visible filesystem on top of it. ZFS can perform many
	sorts of integrity checks on storage pools.

	Virtual Devices (VDEVs)
	A virtual device is the logical storage unit of ZFS. Each VDEV is composed of
	one or more GEOM providers.

	A storage pool contains one or more virtual devices (VDEVs). A VDEV is similar
	to a traditional RAID device. VDEV let you assign specific devices to specific
	roles. ZFS's data redundancy and automated error correction also take place at
	the VDEV level. If pool has sufficient redundancy, ZFS is self-healing.


### Chapter 2: Virtual Devices ###
	Anything that is a FreeBSD GEOM storage provider can became a ZFS storage
	medium: raw disk, disk partition, GEOM classes (GELL, GBDE, high availability,
	labels, multipath, and pluggable schedulers etc).

	GELL (FreeBSD disk encryption subsystem) is the best way to achieve an
	encrypted ZFS pool.

	HAST (FreeBSD distributed storage solution) allows you to mirror a block device
	between computers over the network. Using HAST as the backing storage for a ZFS
	pool allows you to mirror each backing disk to a second machine.

	Multipath high availability, sometimes it is not just the disk dies, but also
	the controller card, the backplane, or the cable. With multipath, enterprise
	drives that are "dual ported" can be connected to more than one HBA. If each
	drive has a path to two different storage controllers, it can survive the loss
	of one of those controllers.

	ZFS supports different type of VDEV (redundancy):
		Plane disks (without redundancy)
		Mirror disks (with redundancy)
		RAIDZ (raidz1, raidz2, raidz3 x number of parity)

	A storage pool consists of one or more VDEVs where the pool data is spread
	across those VDEVs without redundancy. The ZFS pool treats VDEVs as single units
	that provide storage space.

	ZFS stripes the data across each VDEV without redundancy. The individual VDEVs
	provide the redundancy. ZFS supports using mirrored disks and several parity
	based arrays. ZFS uses redundancy to self-heal (automatically).
		Stripe (1 provider)
		Mirrors (2+ providers)
		RAID-Z1 (3+ Providers, 2n+1 disk drives)
		RAID-Z2 (4+ Providers, 2n+2 disk drivers)
		RAID-Z3 (5+ Providers, 2n+3 disk drivers)

	Degraded
	When a provider that belongs to a redundant VDEV fails, the VDEV it is a member
	of becomes "degraded". After the provider is replaced, the system must store
	data on the new provider (mirror copied data and raid-z recalculate data parity).

	RAID-Z versue traditional RAID
	ZFS is the volume manager and the filesystem in addition to the disk redundancy
	layer. ZFS soles 'write hole' issue with copy-on-write and checksums.

	Special VDEVs
	ZIL (ZFS Intent Log), ZFS maintains a ZFS Intent Log as part of the pool. ZFS use
	a SLOG to accelerate writes, with mirror ZIL to prevent data loss.

	LARC, L2ARC (read cache)

### Chapter 3: Pools ###
	ZFS pools form the middle of the ZFS stack, connecting the lower-level virtual
	devices to the user-visible filesystem.
	At the ZFS pool level you can increase the amount of space available to your ZFS
	dataset, or add special virtual devices to improve reading/writing performance.

	A stripe is a chunk of data that's written to a single device. Stripes don't
	provide any redundancy (with mirroring and parity).

	A pool can include multiple VDEVs. Adding VDEVs not only increases the space
	available in the pool but also increases the performance. A pool splits all
	writes between the VDEVs.

	You cannot currently remove a VDEV from a pool. You can remove disks from certain
	types of VDEV (remove a disk from a mirror VDEV). You cannot shrink a pool.

	Pool Alignment (partition alignment: gpart with alignment option)

	ZFS sector size (prefer 4K sectors)
	Using a larger sector size does not impact performance, it does reduce space
	efficiency when you're storing many small files.

	Add following line in /etc/sysctl.conf about ZFS default sector size defines.
		$ sysctl vfs.zfs.min_auto_ashift = 12

	Pool Integrity (ZFS online pool integrity verifies everything)
	ZFS uses hashed almost everywhere, it uses hashes to detect data errors, uses
	pool's redundancy to repair any errors before giving the corrected file to os.
	This is called self-healing.

	While ZFS performs file integrity checks, it also verifies the connections
	between storage blocks (small part of data verification). If you want to perform
	a full integrity check on all data in a pool, scrub it.

	A scrub of a ZFS pool verifies the cryptographic hash of every data block in the
	pool. Scrubs happen while the pool is online and in use.
		$ zpool scrub <pool>

	ZFS features
	Every platform runs OpenZFS, should include a zpool-features manual page that
	lists the pool features this particular install supports.
		$ man zpool-features
		$ zpool get all <pool> |grep feature

### Chapter 4: ZFS Datasets ###
	ZFS pooling free space, giving partitions flexibility impossible with more
	common filesystems. Each dataset has access to all of the free space in the
	pool. You can limit the size of a dataset with a quota or guarantee it a
	minimum amount of space with a reservation.

	ZFS currently has five types of datasets:
		filesystem dataset resembles a traditional filesystem, it stores
			files and directories.
		volume dataset (zvol) is a block device. On ZFS, zvol bypass all
			the overhead of files and directories and reside directly
			on the underlying pool.
		snapshot dataset is a read-only copy of a dataset from a specific
			point in time.
		clone dataset is a new databset based on a snapshot of an existing
			dataset, allowing you to fork a filesystem.

	Compress property
	Most properties apply only to data written after the property is changed.
	The compression property tells ZFS to compress data before writing it to
	disk.

	To get the full benefit of enabling compression, you must rewrite every
	file. You're better off creating a new dataset, copying the data over with
	zfs send, and destroying the original dataset.

	Readonly property
	ZFS uses read-only properties to offer basic information about the dataset.

	Parent/Child Relationships
	We query the compression property on a dataset and all of its children.
		$ zfs get -r compression mystripe
		$ zfs inherit compression mystripe/lamb

	Mountpoint
	Each ZFS filesystem has a mountpoint property that defines where it should
	be mounted. The default mountpoint is built from the pool's mountpoint.

	ZFS volume (zvols) is pretty straightforward--here's a chunk of spaces as
	a block device. You can adjust how a volume uses space and what kind of
	device node it offers.

	FreeBSD normally exposes zvols to the os as geom providers, giving them
	maximum flexibility. Such volumes can be accessed only as raw disk device
	files. They cannot be partitioned or mounted.

	Setting volmode to none means that the volume is not exposed outside ZFS.
	Settting volmode to default means the volume exposure is controlled by the
		sysctl vfs.zfs.vol.mode.
	The value 1 means the default is geom, 2 means dev, and 3 means none.

	Dataset Integrity
	Most of ZFS protections work at the VDEV layer. dataset layer offers some
	redundancy: checksums, metadata redundancy, and copies.
