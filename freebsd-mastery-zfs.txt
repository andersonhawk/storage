### Chapter 0: Introduction ###
	ZFS is not just a filesystem, it is a combination filesystem and volume manager.
	ZFS being aware of exactly where data is going on the disk, can arrange files
	and stripes optimally, from top to bottom. ZFS can use secondary fast storage
	as special-purpose caches, further enhancing performance.


### Chapter 1: Introducing ZFS ###
	The ZFS stack include three layers: filesystem, storage pools, and virtual
	devices. ZFS tightly integrates the filesystem and the lower storage layers.
	This means it can dynamically divide storage space between the various
	filesystems as needed.

	Storage Pools (zpool)
	ZFS uses storage pools rather than disks. A storage pool is an abstraction
	atop the underlying storage providers, letting you seperate the physical
	medium and the user-visible filesystem on top of it. ZFS can perform many
	sorts of integrity checks on storage pools.

	Virtual Devices (VDEVs)
	A virtual device is the logical storage unit of ZFS. Each VDEV is composed of
	one or more GEOM providers.

	A storage pool contains one or more virtual devices (VDEVs). A VDEV is similar
	to a traditional RAID device. VDEV let you assign specific devices to specific
	roles. ZFS's data redundancy and automated error correction also take place at
	the VDEV level. If pool has sufficient redundancy, ZFS is self-healing.


### Chapter 2: Virtual Devices ###
	Anything that is a FreeBSD GEOM storage provider can became a ZFS storage
	medium: raw disk, disk partition, GEOM classes (GELL, GBDE, high availability,
	labels, multipath, and pluggable schedulers etc).

	GELL (FreeBSD disk encryption subsystem) is the best way to achieve an
	encrypted ZFS pool.

	HAST (FreeBSD distributed storage solution) allows you to mirror a block device
	between computers over the network. Using HAST as the backing storage for a ZFS
	pool allows you to mirror each backing disk to a second machine.

	Multipath high availability, sometimes it is not just the disk dies, but also
	the controller card, the backplane, or the cable. With multipath, enterprise
	drives that are "dual ported" can be connected to more than one HBA. If each
	drive has a path to two different storage controllers, it can survive the loss
	of one of those controllers.

	ZFS supports different type of VDEV (redundancy):
		Plane disks (without redundancy)
		Mirror disks (with redundancy)
		RAIDZ (raidz1, raidz2, raidz3 x number of parity)

	A storage pool consists of one or more VDEVs where the pool data is spread
	across those VDEVs without redundancy. The ZFS pool treats VDEVs as single units
	that provide storage space.

	ZFS stripes the data across each VDEV without redundancy. The individual VDEVs
	provide the redundancy. ZFS supports using mirrored disks and several parity
	based arrays. ZFS uses redundancy to self-heal (automatically).
		Stripe (1 provider)
		Mirrors (2+ providers)
		RAID-Z1 (3+ Providers, 2n+1 disk drives)
		RAID-Z2 (4+ Providers, 2n+2 disk drivers)
		RAID-Z3 (5+ Providers, 2n+3 disk drivers)

	Degraded
	When a provider that belongs to a redundant VDEV fails, the VDEV it is a member
	of becomes "degraded". After the provider is replaced, the system must store
	data on the new provider (mirror copied data and raid-z recalculate data parity).

	RAID-Z versue traditional RAID
	ZFS is the volume manager and the filesystem in addition to the disk redundancy
	layer. ZFS soles 'write hole' issue with copy-on-write and checksums.

	Special VDEVs
	ZIL (ZFS Intent Log), ZFS maintains a ZFS Intent Log as part of the pool. ZFS use
	a SLOG to accelerate writes, with mirror ZIL to prevent data loss.

	LARC, L2ARC (read cache)

### Chapter 3: Pools ###
	ZFS pools form the middle of the ZFS stack, connecting the lower-level virtual
	devices to the user-visible filesystem.
	At the ZFS pool level you can increase the amount of space available to your ZFS
	dataset, or add special virtual devices to improve reading/writing performance.

	A stripe is a chunk of data that's written to a single device. Stripes don't
	provide any redundancy (with mirroring and parity).

	A pool can include multiple VDEVs. Adding VDEVs not only increases the space
	available in the pool but also increases the performance. A pool splits all
	writes between the VDEVs.

	You cannot currently remove a VDEV from a pool. You can remove disks from certain
	types of VDEV (remove a disk from a mirror VDEV). You cannot shrink a pool.

	Pool Alignment (partition alignment: gpart with alignment option)

	ZFS sector size (prefer 4K sectors)
	Using a larger sector size does not impact performance, it does reduce space
	efficiency when you're storing many small files.

	Add following line in /etc/sysctl.conf about ZFS default sector size defines.
		$ sysctl vfs.zfs.min_auto_ashift = 12

	Pool Integrity (ZFS online pool integrity verifies everything)
	ZFS uses hashed almost everywhere, it uses hashes to detect data errors, uses
	pool's redundancy to repair any errors before giving the corrected file to os.
	This is called self-healing.

	While ZFS performs file integrity checks, it also verifies the connections
	between storage blocks (small part of data verification). If you want to perform
	a full integrity check on all data in a pool, scrub it.

	A scrub of a ZFS pool verifies the cryptographic hash of every data block in the
	pool. Scrubs happen while the pool is online and in use.
		$ zpool scrub <pool>

	ZFS features
	Every platform runs OpenZFS, should include a zpool-features manual page that
	lists the pool features this particular install supports.
		$ man zpool-features
		$ zpool get all <pool> |grep feature

### Chapter 4: ZFS Datasets ###
	ZFS pooling free space, giving partitions flexibility impossible with more
	common filesystems. Each dataset has access to all of the free space in the
	pool. You can limit the size of a dataset with a quota or guarantee it a
	minimum amount of space with a reservation.

	ZFS currently has five types of datasets:
		filesystem dataset resembles a traditional filesystem, it stores
			files and directories.
		volume dataset (zvol) is a block device. On ZFS, zvol bypass all
			the overhead of files and directories and reside directly
			on the underlying pool.
		snapshot dataset is a read-only copy of a dataset from a specific
			point in time.
		clone dataset is a new databset based on a snapshot of an existing
			dataset, allowing you to fork a filesystem.

	Compress property
	Most properties apply only to data written after the property is changed.
	The compression property tells ZFS to compress data before writing it to
	disk.

	To get the full benefit of enabling compression, you must rewrite every
	file. You're better off creating a new dataset, copying the data over with
	zfs send, and destroying the original dataset.

	Readonly property
	ZFS uses read-only properties to offer basic information about the dataset.

	Parent/Child Relationships
	We query the compression property on a dataset and all of its children.
		$ zfs get -r compression mystripe
		$ zfs inherit compression mystripe/lamb

	Mountpoint
	Each ZFS filesystem has a mountpoint property that defines where it should
	be mounted. The default mountpoint is built from the pool's mountpoint.

	ZFS volume (zvols) is pretty straightforward--here's a chunk of spaces as
	a block device. You can adjust how a volume uses space and what kind of
	device node it offers.

	FreeBSD normally exposes zvols to the os as geom providers, giving them
	maximum flexibility. Such volumes can be accessed only as raw disk device
	files. They cannot be partitioned or mounted.

	Setting volmode to none means that the volume is not exposed outside ZFS.
	Settting volmode to default means the volume exposure is controlled by the
		sysctl vfs.zfs.vol.mode.
	The value 1 means the default is geom, 2 means dev, and 3 means none.

	Dataset Integrity
	Most of ZFS protections work at the VDEV layer. dataset layer offers some
	redundancy: checksums, metadata redundancy, and copies.


### Chapter 5: Repairs & Renovations ###
	VDEVs such as mirrors and RAID-Z are specifically designed to reconstruct
	missing data on damaged disks.

	ZFS knows precisely how much of each disk is in use. When ZFS reassembles
	a replacement storage provider, it copies only the data actually needed 
	on that provider.

	ZFS reconstruction is called resilvering. It takes place only on live
	filesystems. It happends automatically when you replace a storage provider.
	In RAID-Z pool normaly while resilvering, resilvering can greatly slow
	down. Resilvering and scrubbing are performed in order by transaction
	groups, while normal read-write operations are pretty random.

	Expanding Pools
	To increase a pool's size, add a VDEV to the pool. For redundant pools,
	you can replace storage providers with larger providers.

	You can't remove a device from a non-mirror VDEV. You can't add providers
	to any RAID-Z VDEV. To expand a RAID-Z based pool, you must add additional
	VDEVs to the pool, or replace each member disk with a large one.

	You can't add providers to any RAID-Z VDEV. To expand a RAID-Z based pool,
	you must add additional VDEVs to the pool, or replace each member disk with
	a large one.

	You can use zpool attach to expand mirrored and striped VDEVs, but it
	doesn't work on RAID-Z pools. You cann't add providers to a RAID-Z VDEV.

	Hardware Status
		Online, An online pool, VDEV, or provier is working normally.

		Degraded, A degraded pool is missing at least one storage provider.
			the provider is either totally offline, missing, or
			generateing errors more quickly than ZFS tolerates.

		Faulted, faulted storage providers are either corrupt or generate
			more errors than ZFS can tolerate.

		Unavail, means that ZFS can't open the storage provider. Maybe the
			device isn't attached to the system anymore, or perhaps
			it was badly imported.

		Offline, offline devices have been deliberately turned off by the
			sysadmin.

		Removed, hardware lets ZFS set the removed status when a drive
			is pulled. When you reattach the drive, ZFS tries to bring
			the provider back online.

	Log and Cache Device Maintenance
		ZFS Intent Log (write cache)
		L2ARC (read cache)

	Zpool versions and upgrades: pool upgrades are not reversible.
	ZFS versions and feature flags:

### Chapter 6: Disk Space Management ###
	ZFS offers ways to improve disk space utilization (compression, deduplication).
	ZFS flexibility means that users and applications can use disk space if it's
	available.

	A quotea dictates the maximum amount of space a dataset and all its
	descendants can use up. A reservation dictates an amount of space set aside
	for a dataset.

	A quota is a maximum amount of space that a dataset, user, or user group
	may use. All of these quotas are set on a per-dataset basis.

	A quota space including snapshots, child datasets, and everything else. You
	can separately limit the amount of referenced data with a refquota. This
	excludes child datasets and snapshots.

	For example, setting a refquota of 10GB and a quota of 100 GB would tell
	you that you could always have 10 snapshots even if the data completely
	changes.

	Deduplication probably only makes sense when disk space is contrained,
	expensive, and very high performance (SSD cache).

	Your best choice is probably to not use deduplication. Deduplication is
	a great technology, and the people who need it really do need it. Most of
	us don't have deduplicable data, however.


### Chapter 7: Snapshots and Clones ###
	One of ZFS's most powerful features is snapshots. A filesystem or zvol
	snapshot allows you to access a copy of the dataset as it existed at a
	precise moment. Snapshots are read-only, and never change.

	Snapshots are the root of many special ZFS features, such as clones. A clone
	is a fork of a filesystem based on a snapshot. New clones take no additional
	space. As you alter the clone, ZFS allocates new storage to accommodate
	the changes.

	Snapshots also underpin replication, letting you send datasets from one
	host to another. Best of all, ZFS copy-on-write nature means that snapshots
	are free.

	Bookmarks are similiar to snapshots, except they don't keep old data around.

	A clone is a new filesystem created from a snapshot. Clones do not receive
	updates made in the original dataset. They're based on a static snapshot.
