### centos with zfs notes ###


## install zfs packages ##

	# install epel repository #
	$ yum install http://mirrors.ustc.edu.cn/epel/6Server/x86_64/epel-release-6-8.noarch.rpm
	# modify /etc/yum.repos.d/epel.repo with mirrors.ustc.edu.cn repos #
	# install zfs repository #
	$ yum localinstall --nogpgcheck http://archive.zfsonlinux.org/epel/zfs-release.el6.noarch.rpm
	$ yum install kernel-devel zfs




### debian zfs notes ###

## setup zfs build dependenices ##
	$ apt-get install build-essential gawk alien fakeroot linux-headers-$(uname -r)
	$ apt-get install zlib1g-dev uuid-dev libblkid-dev libselinux-dev parted lsscsi wget

## use zfs release packages from zfsonlinux.org ##
## fetch git repository from github.com ##
	$ git clone https://github.com/zfsonlinux/spl.git
	$ git clone https://github.com/zfsonlinux/zfs.git

## config, build  ##
	$ cd spl
	$ ./autogen.sh
	$ ./configure

	$ make deb-utils deb-kmod

	$ cd zfs
	$ ./autogen.sh
	$ ./configure

	$ make deb-utils deb-kmod

## setup spl *.deb packages ##
	$ dpkg -i kmod-spl-3.16.0-4-amd64_0.6.5-29_amd64.deb \
			kmod-spl-devel_0.6.5-29_amd64.deb \
			kmod-spl-devel-3.16.0-4-amd64_0.6.5-29_amd64.deb \
			spl_0.6.5-29_amd64.deb

## setup zfs *.deb packages ##
	$ dpkg -i zfs_0.6.5-66_amd64.deb \
			kmod-zfs-3.16.0-4-amd64_0.6.5-66_amd64.deb \
			libnvpair1_0.6.5-66_amd64.deb \
			libuutil1_0.6.5-66_amd64.deb \
			libzfs2_0.6.5-66_amd64.deb \
			libzpool2_0.6.5-66_amd64.deb

## load zfs kernel module ##
	$ /sbin/modprobe zfs
	$ lsmod |grep zfs

## try to create zpool (raid10) ##
$ zpool create -f mymirror mirror vdb vdc vdd mirror vde vdf vdg
$ zpool status -v
  pool: mymirror
 state: ONLINE
  scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	mymirror    ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    vdb     ONLINE       0     0     0
	    vdc     ONLINE       0     0     0
	    vdd     ONLINE       0     0     0
	  mirror-1  ONLINE       0     0     0
	    vde     ONLINE       0     0     0
	    vdf     ONLINE       0     0     0
	    vdg     ONLINE       0     0     0

errors: No known data errors

## ZFS notes about :https://pthree.org/category/zfs ##
## ZPool Administration ##

# There are seven types of VDEVs in ZFS #
	1. disk (default) - The physical hard drives in system.
	2. file - The absolute path of pre-allocated files/images.
	3. mirror - standard software RAID-1 mirror.
	4. raidz1/2/3 - non-standard distributed partiy-based software RAID levels.
	5. spare - hard drives marked as a 'hot spare' for zfs software RAID.
	6. cache - device used for a level 2 adaptive read cache (L2ARC).
	7. log - a separate log (SLOG) called the 'ZFS Intent Log' or ZIL.

# Raid technology #
# RAID 0: no redundancy #
	it is a way of joining multi-disk together to create one big disk, the
	data is interleaved between the disks and so it improves performance,
	but there is no fault tolerance.

# RAID 1: mirror #
	it uses one disk mirroring the contents of the other disk, the write
	performance is often slightly worse than one disk, but read performance
	is greatly improved as the data is available in two disks and the read
	operation can be split between two disks.

# RAID 1+0: mirrored stripes #
	is is a RAID1 array of two RAID 0 array. it can sustain multiple drive
	failures as long as the two failed drivers are part of the same RAID0 set.
	The read performance is good and the write performance is better than 
	RAID 1.

# Standards Parity RAID5, 6 #
	RAID5 needs a minimum of 3 disks for a RAID-5 array. On two disks, the
	data is striped. A parity bit is then calculated such than the XOR of
	all three stripes in the set is calculated to zero. The parity is then
	written to disk. This allows you to suffer one disk failure.

	'Write hole': write data out and power outtage occurs before parity be
	write.

	RAID5 combine a large number of physical disks, and still maintain some
	redundancy. Both read and write performance usually increased.
	sizeof(RAID 5) = (n - 1) * unit_size.

	RAID6 needs a minimum of 4 disks for a RAID-6 array. it similar to RAID5
	except that data striped with dual parity. The array can recover from two
	disks failure at the same time.
	sizeof(RAID 6) = (n - 2) * unit_size.

# ZFS RAID #
	Rather than the stripe width be statically set at creation, the stripe width
	is dynamic. Every RAIDZ write is a full stripe write, the parity bit is
	flushed with the stripe simultaneously, completely eliminating the RAID5
	write hole.

	ZFS can detect silent errors, and fix them on the fly (Self-healing RAID).
	When the application requests the data, ZFS constructs the strip, compares
	each block against a default checksum in the metadata, which is currently
	fletcher4. If the read stripe does not match the checksum, ZFS finds the
	corrupted block, it then reads the parity, and fixes it through combinatorial
	reconstruction. It then returns good data to the application.

	RAIDZ1 is similar to RAID5, the stripe width is variable.
	RAIDZ2 is similar to RAID6, there is a dual parity bit distributed across
	all the disks in the array.
	RAIDZ3 doesn't have a standardized RAID level to compare it to. it allows
	for three disk failures to maintain data.
	sizeof(RAIDZ3) = (n - 3) * unit_size.

# Hybird RAIDZ #
	Hybird RAIDZ can greatly increase performance, but cost usable disk space.

# ZIL: ZFS Intent Log #
	A logging mechanism where all of the data to be the written is stored, then
	later flushed as a transactional write. Consists of a ZIL header, which 
	points to a list of records, ZIL blocks and a ZIL trailer. The ZIL behaves
	differently for different writes.

# SLOG: Separate Intent Log #
	A separate logging device that caches the synchronous parts of the ZIL
	before flushing them to slower disk.

	SLOG is the device (SSD), ZIL is data on the device. Further, not all
	applications take advantage of the ZIL.

	First and foremost, ZFS has advanced wear-leveling algorithms that will
	evenling wear each chip on the SSD. The is no need for TRIM support, which
	in all reality, is really just a garbage collection support more than
	anything. The wear-leveling of ZFS in inherent due to the copy-on-write
	nature of the filesystem.

	A fast SLOG can provide amazing benefits for applications that need lower
	latencies on synchronous transactions. This works well for database server
	or other applications that are more time sensitive.

