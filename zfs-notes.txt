
### FreeBSD ZFS theory notes ###

# FreeBSD sys/cddl/contrib/opensolaris/uts/common/fs/zfs/zvol.c use GEOM layer
	APIs: BIO_READ, BIO_WRITE, BIO_FLUSH, BIO_DELETE commands.

VDEVs

ZFS uses storage pools (underlying storage providers), seperate the physical
	medium and the user-visible filesytem on the top of it.

zpool contains one or more virtual devices (VDEVs), a VDEV is similar to a
	traditional RAID device. A pool can be arranged for RAID-style 
	redundancy. A dedicated read/write cache provider can improving the
	virtual device's performance.

	ZFS's data redundancy and automated error correction also take place
	at the VDEV level. Everything in zfs is checksummed for integrity 
	verification. zfs is self-healing with sufficient redundancy.

	ZFS uses index blocks and data blocks. ZFS generates index nodes on
	demand. ZFS creates storage blocks in sizes that fit the data. ZFS
	stores multiple copies of crtical index blocks at algorithmically
	predictable locations.

	FreeBSD supports a number of partitioning schemes, but GPT is strongly
	recommended. GPT supports 128 partitions.

	High Availability Storage Technology (HAST) is distributed storage 
	solution, it allows you to mirror a block device between computers
	over network. GEOM supports multipath for high availability, enterprise
	drives are "dual ported" can be connected to more than one HBA.

VDEV is the logical storage unit of ZFS. ZFS supports plain disks, mirrors,
	RAID-Z three types of VDEVs. ZFS pool treats VDEVs as single units that
	provide storage space.

VDEV Redundancy
	Stripe (1 provider) without redundancy.
	Mirrors (2+ providers)
	RAID-Z1 (3+ providers), resembles RAID-5, includes checksumming to 
		ensure file integrity. (2n+1) VDEVs
	RAID-Z2 (4+ providers), resembles RAID-6. (2n+2) VDEVs
	RAID-Z3 (5+ providers), uses three parity disks per VDEV. (2n+3) VDEVs

ZFS RAID advantages (compare traditional RAID solution).
	By combining the filesystem and the volume manager, ZFS can see 
	exactly where all data lies and how the storage layer and the data
	interact.

	ZFS solves 'write hole' problem with copy-on-write and checksums.

	Special VDEVs
		ZFS intent Log (ZIL/SLOG), writes in-progress operations.
		Cache (L2ARC), adaptive replacement cache.

	Performance
		Generally speaking, mirrors can provide better IOPS and read
	bandwidth, but RAID-Z can provide better write bandwidth and much
	better space efficiency.

ZFS Pools (middle of the ZFS stack)
	Connect the lower-level virtual devices to the user-visible file system.
	ZFS doesn't pre-configure special index blocks, it only uses storage
	blocks (stripes). Each block contrains index information to link the 
	block to other blocks on the disk in a tree.

	ZFS uses a special block (uberblock) to store a pointer to the filesystem
	root. ZFS never changes data on the disk, when a block changes, it writes
	a whole new copy of the block with the modified data (copy-on-write).

	A ZFS data pool reserves 128 blocks for uberblock. 

	A ZFS pool "stripes" data across the VDEVs (traditional RAID "stripes" 
	data across the physical devices). A stripe is a chunk of data that's
	written to a single device.

	Stripes do not provice any redundancy. RAID gets redundancy from parity
	and/or mirroring. ZFS pools get any redundancy from the underlying VDEVs.

	ZFS dataset uses a default stripe size of 128KB, but ZFS is smart enough
	to dynamically change that stripe size to fit the equipment and the 
	workload.

	Traditional RAID has a fixed and inflexible data layout, writes to each
	disk in a deterministic order.

	ZFS pools not only tolerate changes, but they're designed to easily accept
	additions as well.

	A ZFS pool can include multiple VDEVs. Adding VDEVs not only increases
	the space available in the pool but also increases the performance.

	Many disks report that they have 512-byte sectors, but they really have
	4096-byte (4K) sectors. The easy way to avoid alignment problems is to
	make all GPT partitions begin and end on megabyte boundaries.

	While using a larger sector size does not impact performance, it does
	reduce space efficiency when you're storing many small files. It is almost
	preferable to force ZFS to use 4K sectors.
		$ sysctl vfs.zfs.min_auto_ashift=12

Create Pools and VDEVs
	Create pools and virtual devices simultaneously with zpool. You'll use
	zpool to add VDEVs to an existing pool and swap out failed devices.

ZFS Datasets (logic volume management)
	A dataset is a named chunk of data: could be a raw block device, 
		a copy of other data, or anything you can cram onto a disk.

	ZFS dataset types: filesystem, volumes, snapshots, clones, and bookmarks.
		Filesystem dataset is a tranditional filesystem (hold file and
			directory, permissions, timestamps, acl etc).

		ZFS volume (zvol) is a block device. Zvols get a device node.

		Snapshot is a read-only copy of a dataset from a specific point
			in time.

		Clone is a new dataset based on a snapshot of an existing 
			dataset. Allowing you to fork a filesystem.

	Many ZFS features (replication, snapshot) operate on a per-dataset
		basis.

	Zvols support spare volumes (thin provisioning): is a volume where
		the reservation is less than the volume size. FreeBSD exposes
		zvols to the os as geom providers.

	ZFS protections work at the VDEV layer. You can do somethings at the
		dataset layer to offer some redundancy (checksums, metadata
		redundancy, copies).
